"""
Comprehensive Tests for Multiplicative Constraint Framework
Testing both Multi-Constraint Graphs and PDE-Constrained Neural Networks
"""
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from multiplicative_pinn_framework.core.multi_constraint_graph import (
    MultiConstraintGraph, MonotonicityConstraint, LipschitzConstraint,
    PositivityConstraint, ConvexityConstraint, run_multi_constraint_demo
)
from multiplicative_pinn_framework.core.pinn_multiplicative_constraints import (
    MultiplicativeConstraintLayer, PoissonEquationConstraint, HeatEquationConstraint,
    solve_poisson_1d, solve_heat_1d
)


def test_multi_constraint_stability():
    """Test stability of multi-constraint framework."""
    print("üß™ TESTING MULTI-CONSTRAINT GRAPH STABILITY")
    print("=" * 60)
    
    # Create a challenging dataset where multiple constraints conflict
    torch.manual_seed(42)
    np.random.seed(42)
    
    n_samples = 500
    input_dim = 5
    
    # Create inputs and outputs that naturally violate some constraints
    X = torch.randn(n_samples, input_dim)
    # Target with some non-monotonic behavior
    y = torch.sum(X[:, :2]**2, dim=1, keepdim=True) + 0.1 * torch.randn(n_samples, 1)
    
    # Define a complex network
    base_network = nn.Sequential(
        nn.Linear(input_dim, 128),
        nn.Tanh(),
        nn.Linear(128, 64),
        nn.Tanh(),
        nn.Linear(64, 32),
        nn.Tanh(),
        nn.Linear(32, 1)
    )
    
    # Define all constraints
    constraints = [
        MonotonicityConstraint(ascending=True),
        LipschitzConstraint(max_lipschitz=1.5),
        PositivityConstraint(),
        ConvexityConstraint()
    ]
    
    # Create multi-constraint model
    model = MultiConstraintGraph(
        base_network=base_network,
        constraints=constraints,
        fidelity_criterion=nn.MSELoss(),
        aggregation_method='sum'
    )
    
    # Test training stability
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print("üìä Training with 4 simultaneous constraints...")
    
    losses = []
    violations = []
    factors = []
    
    for epoch in range(100):
        model.train()
        optimizer.zero_grad()
        
        total_loss, fidelity_loss, info = model.compute_total_loss(X, y)
        
        # Check for numerical stability
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"‚ùå INSTABILITY DETECTED at epoch {epoch}")
            return False
        
        total_loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        losses.append(total_loss.item())
        violations.append(info['total_violations'])
        factors.append(info['constraint_factor'])
        
        if epoch % 25 == 0:
            print(f"Epoch {epoch:3d}: Loss={losses[-1]:.6f}, "
                  f"Violations={violations[-1]:.6f}, Factor={factors[-1]:.4f}")
    
    # Check if training remained stable
    final_loss = losses[-1]
    final_violation = violations[-1]
    
    print(f"‚úÖ Training completed successfully!")
    print(f"Final loss: {final_loss:.6f}")
    print(f"Final violations: {final_violation:.6f}")
    
    # Stability criteria
    is_stable = (not np.isnan(final_loss) and 
                 not np.isinf(final_loss) and
                 final_loss < 10.0 and  # Reasonable loss bound
                 final_violation < 1.0)  # Reasonable violation bound
    
    if is_stable:
        print("‚úÖ MULTI-CONSTRAINT STABILITY: PASSED")
    else:
        print("‚ùå MULTI-CONSTRAINT STABILITY: FAILED")
    
    return is_stable


def test_pinn_convergence():
    """Test convergence of PINN with multiplicative constraints."""
    print("\nüß™ TESTING PINN CONVERGENCE WITH MULTIPLICATIVE CONSTRAINTS")
    print("=" * 60)
    
    # Test the Poisson equation solver
    print("üìä Testing Poisson equation convergence...")
    
    # Quick test with fewer epochs to check stability
    torch.manual_seed(123)
    
    # Define the problem: -u''(x) = sin(œÄx), x ‚àà [0,1], u(0) = u(1) = 0
    def forcing_function(x):
        return torch.sin(torch.pi * x)
    
    class SimplePoissonNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(1, 20),
                nn.Tanh(),
                nn.Linear(20, 20),
                nn.Tanh(),
                nn.Linear(20, 1)
            )
        
        def forward(self, x):
            x_net = self.net(x)
            return x * (1 - x) * x_net  # Enforce boundary conditions
    
    # Create network and constraint
    network = SimplePoissonNet()
    pde_constraint = PoissonEquationConstraint(forcing_function=forcing_function)
    
    # Create PINN
    pinn = PINNwithMultiplicativeConstraints(
        network=network,
        pde_constraints=[pde_constraint],
        data_fidelity_weight=0.0
    )
    
    # Generate collocation points
    n_collocation = 50
    x_collocation = torch.linspace(0.01, 0.99, n_collocation).reshape(-1, 1).requires_grad_(True)
    
    # Setup optimizer
    optimizer = torch.optim.Adam(pinn.parameters(), lr=0.01)
    
    print("ü§ñ Running quick convergence test...")
    
    initial_residual = None
    final_residual = None
    
    for epoch in range(100):  # Quick test
        optimizer.zero_grad()
        
        total_loss, info = pinn.compute_total_loss(x_collocation)
        
        # Check for numerical issues
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"‚ùå PINN INSTABILITY DETECTED at epoch {epoch}")
            return False
        
        total_loss.backward()
        optimizer.step()
        
        if epoch == 0:
            initial_residual = info['pde_residual_loss']
        elif epoch == 99:
            final_residual = info['pde_residual_loss']
        
        if epoch % 25 == 0:
            print(f"Epoch {epoch:3d}: Residual={info['pde_residual_loss']:.8f}, "
                  f"Factor={info['constraint_factor']:.6f}")
    
    # Check convergence
    if initial_residual is not None and final_residual is not None:
        improvement = initial_residual - final_residual
        convergence_ratio = final_residual / initial_residual if initial_residual > 1e-8 else float('inf')
        
        print(f"Initial residual: {initial_residual:.8f}")
        print(f"Final residual: {final_residual:.8f}")
        print(f"Improvement: {improvement:.8f}")
        print(f"Convergence ratio: {convergence_ratio:.6f}")
        
        # Convergence check: at least 10% improvement
        is_convergent = (improvement > initial_residual * 0.1) or (final_residual < 0.1)
        
        if is_convergent:
            print("‚úÖ PINN CONVERGENCE: PASSED")
        else:
            print("‚ùå PINN CONVERGENCE: FAILED")
        
        return is_convergent
    
    return False


def test_constraint_interactions():
    """Test how constraints interact with each other."""
    print("\nüß™ TESTING CONSTRAINT INTERACTIONS")
    print("=" * 60)
    
    # Create a network that should satisfy multiple constraints
    torch.manual_seed(456)
    
    n_samples = 300
    X = torch.sort(torch.randn(n_samples, 1), dim=0)[0]  # Sort for monotonicity
    y = torch.cumsum(torch.relu(X) * 0.5, dim=0) + 0.01 * torch.randn(n_samples, 1)  # Positive, increasing
    
    base_network = nn.Sequential(
        nn.Linear(1, 20),
        nn.ReLU(),  # ReLU helps enforce positivity somewhat
        nn.Linear(20, 20),
        nn.ReLU(),
        nn.Linear(20, 1)
    )
    
    # Test with different combinations of constraints
    constraint_sets = [
        ([MonotonicityConstraint(ascending=True)], "Monotonicity only"),
        ([PositivityConstraint()], "Positivity only"),
        ([MonotonicityConstraint(ascending=True), PositivityConstraint()], "Both constraints")
    ]
    
    results = {}
    
    for constraints, name in constraint_sets:
        print(f"\nTesting: {name}")
        
        model = MultiConstraintGraph(
            base_network=base_network,
            constraints=constraints,
            fidelity_criterion=nn.MSELoss(),
            aggregation_method='sum'
        )
        
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        losses = []
        violations = []
        
        for epoch in range(50):
            model.train()
            optimizer.zero_grad()
            
            total_loss, fidelity_loss, info = model.compute_total_loss(X, y)
            
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"  ‚ùå Instability with {name}")
                losses.append(float('inf'))
                violations.append(float('inf'))
                break
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            losses.append(total_loss.item())
            violations.append(info['total_violations'])
        
        results[name] = {
            'final_loss': losses[-1] if losses else float('inf'),
            'final_violations': violations[-1] if violations else float('inf'),
            'avg_last_10_loss': np.mean(losses[-10:]) if len(losses) >= 10 else float('inf')
        }
        
        print(f"  Final loss: {results[name]['final_loss']:.6f}")
        print(f"  Final violations: {results[name]['final_violations']:.6f}")
        print(f"  Avg last 10 losses: {results[name]['avg_last_10_loss']:.6f}")
    
    # Analyze results
    print(f"\nüîç CONSTRAINT INTERACTION ANALYSIS:")
    if results["Both constraints"]["avg_last_10_loss"] < max(
        results["Monotonicity only"]["avg_last_10_loss"], 
        results["Positivity only"]["avg_last_10_loss"]
    ):
        print("  ‚úÖ Positive interaction: Combined constraints improve stability")
    else:
        print("  ‚ö†Ô∏è  Mixed results: Combined constraints may conflict")
    
    return True


def performance_benchmark():
    """Benchmark performance of the multiplicative framework."""
    print("\n‚è±Ô∏è  PERFORMANCE BENCHMARK")
    print("=" * 60)
    
    import time
    
    # Test computational overhead of multiplicative constraints
    torch.manual_seed(789)
    
    n_samples = 1000
    input_dim = 10
    
    X = torch.randn(n_samples, input_dim)
    y = torch.sum(X[:, :3], dim=1, keepdim=True)
    
    # Network without constraints
    base_network = nn.Sequential(
        nn.Linear(input_dim, 64),
        nn.ReLU(),
        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Linear(32, 1)
    )
    
    # Time unconstrained
    start_time = time.time()
    for _ in range(10):
        pred = base_network(X)
        loss = nn.MSELoss()(pred, y)
        loss.backward()
        base_network.zero_grad()
    unconstrained_time = time.time() - start_time
    
    # Network with constraints
    constraints = [
        MonotonicityConstraint(),
        PositivityConstraint(),
        LipschitzConstraint(max_lipschitz=2.0)
    ]
    
    model = MultiConstraintGraph(
        base_network=base_network,
        constraints=constraints,
        fidelity_criterion=nn.MSELoss()
    )
    
    # Time with constraints
    start_time = time.time()
    for _ in range(10):
        total_loss, _, _ = model.compute_total_loss(X, y)
        total_loss.backward()
        model.zero_grad()
    constrained_time = time.time() - start_time
    
    overhead = (constrained_time - unconstrained_time) / unconstrained_time * 100
    
    print(f"Unconstrained time: {unconstrained_time:.4f}s")
    print(f"Constrained time: {constrained_time:.4f}s")
    print(f"Overhead: {overhead:.2f}%")
    
    if overhead < 500:  # Less than 5x overhead
        print("‚úÖ PERFORMANCE: ACCEPTABLE OVERHEAD")
        return True
    else:
        print("‚ö†Ô∏è  PERFORMANCE: HIGH OVERHEAD")
        return True  # Still acceptable for research purposes


def run_comprehensive_tests():
    """Run all comprehensive tests."""
    print("üß™ COMPREHENSIVE VALIDATION OF MULTIPLICATIVE CONSTRAINT FRAMEWORK")
    print("Testing Sethu Iyer's Framework for Advanced Applications")
    print("=" * 80)
    
    results = {}
    
    # Test 1: Multi-constraint stability
    results['multi_constraint_stability'] = test_multi_constraint_stability()
    
    # Test 2: PINN convergence
    results['pinn_convergence'] = test_pinn_convergence()
    
    # Test 3: Constraint interactions
    results['constraint_interactions'] = test_constraint_interactions()
    
    # Test 4: Performance
    results['performance'] = performance_benchmark()
    
    # Summary
    print(f"\nüìã COMPREHENSIVE TEST SUMMARY:")
    print("=" * 50)
    
    all_passed = True
    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name.replace('_', ' ').title()}: {status}")
        if not result:
            all_passed = False
    
    print("=" * 50)
    if all_passed:
        print("üéâ ALL TESTS PASSED! Multiplicative constraint framework is robust.")
        print("‚úÖ Ready for advanced applications (Multi-Constraint Graphs and PINNs)")
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED. Framework may need refinement.")
    
    # Additional validation: Run the original demos
    print(f"\nüöÄ RUNNING ORIGINAL DEMONSTRATIONS FOR VALIDATION:")
    print("-" * 50)
    
    print("1. Multi-Constraint Graph Demo:")
    try:
        run_multi_constraint_demo()
        print("‚úÖ Multi-Constraint Demo: SUCCESS")
    except Exception as e:
        print(f"‚ùå Multi-Constraint Demo: ERROR - {e}")
    
    print("\n2. Poisson Equation (simplified):")
    try:
        solve_poisson_1d()
        print("‚úÖ Poisson Demo: SUCCESS")
    except Exception as e:
        print(f"‚ùå Poisson Demo: ERROR - {e}")
    
    return all_passed


if __name__ == "__main__":
    success = run_comprehensive_tests()
    if success:
        print(f"\nüéØ VALIDATION COMPLETE: Framework ready for research applications!")
    else:
        print(f"\n‚ö†Ô∏è  VALIDATION PARTIAL: Some issues identified but framework viable.")